{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f8c1a5",
   "metadata": {},
   "source": [
    "### Implementação : Preditor de Artigos de Palavras em Alemão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687885f",
   "metadata": {},
   "source": [
    "O modelo proposto tem como objetivo predizer o artigo de substantivos em alemão (Der - Masculino, Die - Feminino, Das - Neutro), cuja tarefa é bastante desafiadora para estudantes do idioma alemão pela falta de correlação dos gêneros das palavras em relação ao idioma português.\n",
    "\n",
    "O objetivo desta implementação possui dois direcionamentos:\n",
    "\n",
    "- Confirmação de padrões observados no idioma em relação à distribuição dos caracteres em uma palavra para determinado gênero.\n",
    "- Determinação do gênero mais dificil de predizer por conta da presença de exceções presentes no idioma\n",
    "- Discussão sobre memorização e dificuldade de memorização dos gêneros das palavras algoritmo x humano\n",
    "\n",
    "Além da predição propriamente dita, a diferença de acurácia gerada neste modelo nos proverá insights valiosos sobre a presença de exceções às regras em relação às terminações das palavras em alemão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401e09e",
   "metadata": {},
   "source": [
    "#### 1 - Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a5d6f",
   "metadata": {},
   "source": [
    "Importaremos as bibliotecas necessárias para o projeto. Esta inclui numpy, pandas, pickle, scikit-learn e keras (tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db123ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Dense, Flatten\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4670c1",
   "metadata": {},
   "source": [
    "#### 2 - Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a57121",
   "metadata": {},
   "source": [
    "Os dados estão serializados em formato json, e constituem de 2610 palavras alemãs com seus respectivos gêneros, balanceados de forma que contenha quantidades iguais para cada um\n",
    "Para a extração, carregamos o arquivo serializado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d83d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    infile = open(filename,'rb')\n",
    "    objeto = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return objeto\n",
    "\n",
    "base = load_pickle('Worter.p')\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e4282",
   "metadata": {},
   "source": [
    "#### 3 - Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cce4e2",
   "metadata": {},
   "source": [
    "##### 3.1 - Tratamento dos Caracteres\n",
    "\n",
    "Devemos extrair somente as palavras e seus artigos, e armazena-los em um DataFrame Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b114315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract words and articles\n",
    "artikels = []\n",
    "worter = []\n",
    "\n",
    "for key, wort in base.items():\n",
    "    artikels.append(wort['Gender'])\n",
    "    worter.append(wort['ORTH'])\n",
    "    \n",
    "df = pd.DataFrame({'artikel': artikels,'wort': worter})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c06393",
   "metadata": {},
   "source": [
    "Após armazenamento, devemos garantir que todas as palavras possuam somente letras minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Dataset\n",
    "df = df.loc[(df.loc[:,'artikel'] == 'Der') | \n",
    "            (df.loc[:,'artikel'] == 'Die') |\n",
    "            (df.loc[:,'artikel'] == 'Das'), :] \n",
    "\n",
    "df['wort'] = df['wort'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e322621",
   "metadata": {},
   "source": [
    "##### 3.2 - Treinamento e Teste\n",
    "\n",
    "Para evitar a contaminação dos dados, separamos dois conjuntos de dados, sendo treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d02eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_texts = train_df['wort'].values \n",
    "test_texts = test_df['wort'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4406eb",
   "metadata": {},
   "source": [
    "##### 3.3 Tokenizador\n",
    "\n",
    "Criamos um Tokenizer do Keras para trabalhar em nível de caracteres, onde cada Token equivale a um caractere. Treinando com os caracteres presentes nas palavras de treinamento, obtemos um dicionário de caracteres, incluindo as variantes alemãs ä, ö, ü e ß, e UNK para caracteres desconhecidos (oov_token como parâmetro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed67208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09c428",
   "metadata": {},
   "source": [
    "Checamos o tamanho do dicionário gerado, incluindo as 26 letras do alfabeto, 4 variantes alemãs e 1 UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc797ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f68a9",
   "metadata": {},
   "source": [
    "##### 3.4 Texto para Sequências\n",
    "\n",
    "Os textos de treinamento e teste são convertidos em sequências de números inteiros utilizando os tokens gerados acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index \n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)\n",
    "test_texts[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a787e26",
   "metadata": {},
   "source": [
    " ##### 3.5 Padding\n",
    " \n",
    " As sequências de números inteiros dos textos de treinamento e teste são padronizadas para terem o mesmo comprimento máximo, definido como 25. Tanto as sequências de treinamento quanto as de teste são ajustadas para terem um comprimento máximo de 25, preenchendo os valores com zeros no início (padding='pre', garantindo que todas as sequências tenham o mesmo comprimento e que contemplem palavras grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "maxlen = 25\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=maxlen, padding='pre')\n",
    "test_data = pad_sequences(test_texts, maxlen=maxlen, padding='pre')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fe986",
   "metadata": {},
   "source": [
    "##### 3.6 - Conversão para matriz Float\n",
    "\n",
    "As sequências de treinamento e teste são convertidas em matrizes numpy do tipo float, a fim de compartibilizar com o modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb585a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778cefa",
   "metadata": {},
   "source": [
    "##### 3.7 - Tratamento das Classes\n",
    "\n",
    "As classes dos conjuntos de treinamento e teste são então definidos como inteiros através do LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_df['artikel'].values\n",
    "test_classes = test_df['artikel'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le = le.fit(df['artikel'])\n",
    "\n",
    "train_classes = le.transform(train_classes)\n",
    "test_classes = le.transform(test_classes)\n",
    "test_classes[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78434be",
   "metadata": {},
   "source": [
    "As classes são então canonizadas através de OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec94065",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = to_categorical(train_classes)\n",
    "test_classes = to_categorical(test_classes)\n",
    "test_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14625df",
   "metadata": {},
   "source": [
    "##### 3.8 - Atribuição dos Pesos\n",
    "\n",
    "Construímos assim os pesos da camada de embedding para a rede neural. Em seguida, iteramos sobre cada palavra e índice no índice de palavras gerado pelo Tokenizer. Para cada palavra, é criado um vetor one-hot de zeros, onde a posição correspondente ao índice da palavra é definida como 1, para que seja convertida em uma matriz numpy, tornando-se os pesos da camada de embedding da rede neural. Esses pesos serão usados ​​para inicializar a camada de embedding da rede neural durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setar onehot para cada letra\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "embedding_weights[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55066c",
   "metadata": {},
   "source": [
    "#### 4 - Treinamento do Modelo\n",
    "\n",
    "Criamos a função responsável por criar e compilar o modelo da rede neural. Ela recebe o tamanho do vocabulário e o comprimento máximo das sequências. O modelo consiste em uma camada de embedding inicializada com os pesos predefinidos (embedding_weights), seguida por uma camada LSTM com ativação ReLU, uma camada Flatten, uma camada Dense com ativação ReLU e dropout, e finalmente uma camada Dense de saída com ativação softmax para a classificação em 3 classes. O modelo é compilado com a função de perda categorical_crossentropy e o otimizador adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter \n",
    "\n",
    "embedding_size = 31\n",
    "num_of_classes = 3\n",
    "\n",
    "def save_pickle(filename, objeto):\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(objeto,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):  \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(maxlen,)))\n",
    "    model.add(Embedding(vocabulary_size, seq_len, weights=[embedding_weights], input_length=maxlen))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_of_classes,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    \n",
    "    return model\n",
    "\n",
    "model = create_model(vocab_size+1, embedding_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949685f7",
   "metadata": {},
   "source": [
    "São definidos dois callbacks para monitorar e ajustar o treinamento da rede neural: EarlyStopping e ReduceLROnPlateau\n",
    "\n",
    "O primeiro monitora onloss durante o treinamento. Se a perda não diminuir após 10 épocas, o treinamento será interrompido e os pesos da melhor época serão restaurados (restore_best_weights=True).\n",
    "\n",
    "O segundo callback, também monitora a perda. No entanto, se a perda não diminuir após 3 épocas, a taxa de aprendizado será reduzida em um fator de 0.1 (factor=0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe331c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor= 'loss', patience = 10, verbose = 1, restore_best_weights=True)\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor= 0.1, patience= 3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d117606",
   "metadata": {},
   "source": [
    "Por fim, o modelo é treinado com tamanho de lote 32 e por 50 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento do modelo\n",
    "history = model.fit(train_data, train_classes,\n",
    "                    validation_data=(test_data, test_classes),\n",
    "                    batch_size=32,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    callbacks=[es, rlr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69877dfa",
   "metadata": {},
   "source": [
    "##### 5 - Avaliação dos dados de Teste\n",
    "\n",
    "O modelo treinado é avaliado utilizando os dados de teste, obtendo assim a sua perda e acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(test_data,test_classes)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d12c4",
   "metadata": {},
   "source": [
    "Geramos as predições para cada exemplo nos dados de teste\n",
    "\n",
    "Em seguida, as previsões são convertidas em rótulos de classe usando np.argmax() para encontrar o índice da classe com a maior probabilidade para cada exemplo.\n",
    "\n",
    "Depois, os rótulos de classe são invertidos para seus valores originais para revertar a canonização realizada durante o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84980e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions = [np.argmax(x) for x in predictions]\n",
    "predictions = le.inverse_transform(predictions)\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5ddc2",
   "metadata": {},
   "source": [
    "Assim, geramos o DataFrame com os artigos reais e os preditos pelo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Wort': test_df['wort'].values, 'Real Artikel': test_df['artikel'].values, 'Predicted Artikel': predictions})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5116f7",
   "metadata": {},
   "source": [
    "#### 6 - Particionamento da Solução e Discussão dos Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbabbcf",
   "metadata": {},
   "source": [
    "Obtemos as acurácias para cada um dos artigos Der, Die, Das\n",
    "\n",
    "No idioma alemão, determinados gêneros possuem menos exceções às regras do que outros.\n",
    "\n",
    "Sendo assim, espera-se que os resultados sejam na seguinte ordem de acurácia: Die > Der > Das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_der = results.loc[results['Real Artikel'] == 'Der']\n",
    "accuracy_der = accuracy_score(results_der['Real Artikel'], results_der['Predicted Artikel'])\n",
    "accuracy_der\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e42df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_die = results.loc[results['Real Artikel'] == 'Die']\n",
    "accuracy_die = accuracy_score(results_die['Real Artikel'], results_die['Predicted Artikel'])\n",
    "accuracy_die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_das = results.loc[results['Real Artikel'] == 'Das']\n",
    "accuracy_das = accuracy_score(results_das['Real Artikel'], results_das['Predicted Artikel'])\n",
    "accuracy_das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea0fa5",
   "metadata": {},
   "source": [
    "Os resultados gerais de acurácia parecem um pouco desmotivadores, principalmente para o artigo neutro Das, que apresentou mais erros do que acertos.\n",
    "\n",
    "Isso é esperado, uma vez que o idioma alemão apresenta diversas exceções às regras\n",
    "\n",
    "No entanto, podemos observar alguns padrões de comportamento das palavras de acordo com o caractere e suas posições.\n",
    "\n",
    "Filtramos assim as palavras com determinadas terminações, e calculamos a sua acurácia separadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ending(wort_ending):\n",
    "    result_textpart = results.loc[results['Wort'].str.endswith(wort_ending)]\n",
    "    accuracy_textpart = accuracy_score(result_textpart['Real Artikel'], result_textpart['Predicted Artikel'])\n",
    "    print(f'Accuracy: \"{wort_ending}\": {round(accuracy_textpart,2)}')\n",
    "\n",
    "custom_ending('er') #Der\n",
    "custom_ending('en') #Der\n",
    "custom_ending('keit') #Die\n",
    "custom_ending('heit') #Die\n",
    "custom_ending('tät') #Die\n",
    "custom_ending('e') #Die\n",
    "custom_ending('chen') #Das\n",
    "custom_ending('a') #Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b65de6",
   "metadata": {},
   "source": [
    "Os resultados de acurácia foram esperados, pois:\n",
    "- Palavras com terminações er e en costumam ser masculinas, com muitas exceções às regras, tendendo a diminuir o acerto pelo modelo\n",
    "- Todas as palavras terminadas com \"keit\", \"heit\" e \"tät\" são predominantemente femininas, sem exceções, tendendo ao 100% de acerto. Terminações com \"e\" apresenta algumas exceçoes.\n",
    "- Palavras terminadas com \"chen\" e \"a\" costumam ser palavras de gênero neutro, com poucas exceções. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a825f6",
   "metadata": {},
   "source": [
    "#### 7 - Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb6a9e",
   "metadata": {},
   "source": [
    "Estudantes do idioma alemão se deparam com palavras que não seguem a mesma lógica de atribuição de gênero do português, buscando as vezes por padrões nas palavras que fornecem pistas para esta tarefa\n",
    "\n",
    "Porém, o esforço atribuído nesta memorização pode não ser compensada, uma vez que este padrão pode apresentar muitas exceções às regras, como os artigos \"er\" e \"en\", enquanto que outro padrões podem possuir comportamento mais bem definido, como heit e keit. Este comportamento foi refletido no modelo treinado.\n",
    "\n",
    "Assim, o modelo auxilia o aluno à a desenvolver uma compreensão mais profunda das nuances da língua alemã e a reconhecer quando as regras podem não se aplicar de forma estrita."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
